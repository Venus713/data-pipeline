service: data-pipeline-4byte-software
frameworkVersion: '2'

custom:
  projectName: 4byte-software
  documentsTableName: ${self:custom.projectName}-data
  s3bucketName: ${self:custom.projectName}-csv-data
  tableThroughput: 10
  pythonRequirements:
    dockerizePip: non-linux
    layer: true

provider:
  name: aws
  runtime: python3.8
  lambdaHashingVersion: 20201221
  stage: dev
  region: us-east-1
  environment:
    DB_TABLE_NAME: ${self:custom.documentsTableName}
    S3_BUCKET_NAME: ${self:custom.s3bucketName}
    REGION: ${self:provider.region}

  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource:
        - "Fn::GetAtt": [ DataPipelineTable, Arn ]
    - Effect: 'Allow'
      Action:
        - 's3:PutObject'
        - 's3:GetObject'
        - 's3:ListBucket'
      Resource: 
        - "arn:aws:s3:::${self:custom.s3bucketName}/*"
        - "arn:aws:s3:::${self:custom.s3bucketName}"

plugins:
  - serverless-python-requirements
  - serverless-offline
functions:
  import_csv_to_db:
    handler: src.handler.import_csv_to_db
    events:
      - s3:
          bucket: ${self:custom.s3bucketName}
          event: s3:ObjectCreated:Put
          rules:
            - suffix: .csv

resources:
  - ${file(resources/dynamodb-table.yml)}
  # - ${file(resources/s3-bucket.yml)}
